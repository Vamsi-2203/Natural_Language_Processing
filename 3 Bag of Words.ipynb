{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b895a39-988a-4ca7-b6e8-286a53a15cf9",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)\n",
    "- The Bag of Words (BoW) model is a fundamental method used in Natural Language Processing (NLP) to convert text data into numerical representations.\n",
    "- This approach disregards grammar and word order but considers the frequency or presence of words in a document.\n",
    "- It is particularly useful for text classification, sentiment analysis, and information retrieval tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921907c1-09be-49cd-905f-6e1e4fb269dd",
   "metadata": {},
   "source": [
    "## Key Concepts of Bag of Words\n",
    "- **Vocabulary:** The set of all unique words in the corpus.\n",
    "- **Vector Representation:** Each document is represented as a vector of word frequencies or binary indicators of word presence.\n",
    "- **Simplicity:** The model is easy to understand and implement, making it a good starting point for text analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b016f927-1dcf-4283-bdb0-4dbbd04d936d",
   "metadata": {},
   "source": [
    "## Steps to Create a Bag of Words Model\n",
    "- **Tokenization:** Splitting the text into words or tokens.\n",
    "- **Building the Vocabulary:** Creating a list of all unique tokens in the corpus.\n",
    "- **Vectorization:** Representing each document as a vector of word counts or binary values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e77ee77-52eb-47c0-95b9-cce5bb4246ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6a6916d-39e7-49a6-bbec-1bae205e8868",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc36fda-7f5b-40fe-9675-97e18a1799ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8357c60-0c34-43ed-9b87-7779272466b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['.', 'and', 'animals', 'are', 'beautiful', 'cats', 'dogs', 'friendly', 'loyal', 'pets', 'popular']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10060] A\n",
      "[nltk_data]     connection attempt failed because the connected party\n",
      "[nltk_data]     did not properly respond after a period of time, or\n",
      "[nltk_data]     established connection failed because connected host\n",
      "[nltk_data]     has failed to respond>\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Cats are beautiful animals.\",\n",
    "    \"Dogs are loyal and friendly animals.\",\n",
    "    \"Cats and dogs are popular pets.\"\n",
    "]\n",
    "\n",
    "# Tokenize the documents (you may add more preprocessing steps like stemming or stop word removal)\n",
    "nltk.download('punkt')\n",
    "tokenized_docs = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Flatten the list of lists and remove duplicates to create a vocabulary\n",
    "vocabulary = sorted(set([word for doc in tokenized_docs for word in doc]))\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "573824c0-304b-4bfb-b451-110d199b72a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Model:\n",
      " [[0 1 1 1 1 0 0 0 0 0]\n",
      " [1 1 1 0 0 1 1 1 0 0]\n",
      " [1 0 1 0 1 1 0 0 1 1]]\n",
      "Feature Names:\n",
      " ['and' 'animals' 'are' 'beautiful' 'cats' 'dogs' 'friendly' 'loyal' 'pets'\n",
      " 'popular']\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Building the Vocabulary and Vectorization\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents to create the bag of words model\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert the result to an array for better readability\n",
    "bow_array = X.toarray()\n",
    "\n",
    "print(\"Bag of Words Model:\\n\", bow_array)\n",
    "print(\"Feature Names:\\n\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9750b1-117f-4ee9-9d96-756856f2774a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
