{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79261b8c-ab91-4623-9580-6df5f25aef4b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "- Sentiment Analysis, also known as opinion mining, is a technique used in natural language processing (NLP) to determine the sentiment expressed in a piece of text. \n",
    "- The sentiment can be positive, negative, or neutral. This technique is widely used in various applications such as customer feedback analysis, social media monitoring, market research, and more.\n",
    "\n",
    "### Key Concepts in Sentiment Analysis:\n",
    "\n",
    "- **Polarity:** Indicates whether the sentiment is positive, negative, or neutral.\n",
    "- **Subjectivity:** Measures how subjective or objective a piece of text is.\n",
    "- **Feature Extraction:** Converts text into numerical representations, often using techniques like Bag-of-Words (BoW), Term Frequency-Inverse Document Frequency (TF-IDF), or word embeddings.\n",
    "- **Classification Model:** A machine learning model trained to predict the sentiment of a given text based on extracted features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a119f3-82a1-44a9-8169-99ac8f2e946f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11e92aa8-af70-43ce-b428-c45887a0fdfa",
   "metadata": {},
   "source": [
    "# Implementation of Sentiment Analysis using TextBlob:\n",
    "TextBlob is a simple library for processing textual data. It provides an easy-to-use API for common NLP tasks including sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90a5adc7-5e42-49ea-9588-1203fe3f475d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\user\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/626.3 kB ? eta -:--:--\n",
      "   - ------------------------------------- 30.7/626.3 kB 435.7 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 92.2/626.3 kB 880.9 kB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 450.6/626.3 kB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 626.3/626.3 kB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Install TextBlob\n",
    "#First, you need to install the TextBlob library:\n",
    "\n",
    "!pip install textblob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4cd8f1f-a161-4eeb-bd36-4c2b9f28d128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7960630e-77d3-44c1-b33c-1e0fa1c8df56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity: 0.6125\n",
      "Subjectivity: 0.75\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Perform Sentiment Analysis\n",
    "#Here's how to use TextBlob for sentiment analysis:\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Sample text\n",
    "text = \"I love this product! It's absolutely amazing.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Get the sentiment\n",
    "sentiment = blob.sentiment\n",
    "\n",
    "print(f\"Polarity: {sentiment.polarity}\")\n",
    "print(f\"Subjectivity: {sentiment.subjectivity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79ab643-a674-4cf3-8186-e715df2a3643",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f30bb58-d2ff-4b3e-a79d-42b17850c3d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bd6665-904e-4f54-8c7c-68392e9b69d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6d694f7-ce46-4c08-8e91-224c2cfc2fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.4)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "#Step 1: Install Required Libraries\n",
    "#Make sure to install the necessary libraries if you haven't already:\n",
    "!pip install nltk scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc2598c8-a932-4185-96bc-bbf244315321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Step 2: Import Libraries and Load Data\n",
    "#We'll use the movie_reviews dataset from the nltk library, which contains positive and negative movie reviews.\n",
    "import nltk\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "\n",
    "# Download the movie_reviews dataset if not already downloaded\n",
    "nltk.download('movie_reviews')\n",
    "\n",
    "# Load the movie reviews dataset\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "# Shuffle the documents to mix positive and negative reviews\n",
    "random.shuffle(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53e83b81-e0ad-4c10-ad79-f1beeda6b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3: Preprocess the Text Data\n",
    "#Convert the text into a format suitable for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3c41a2c-37da-4643-aee6-bad2d8d267d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Get the list of all words in the dataset\n",
    "all_words = FreqDist(word.lower() for word in movie_reviews.words())\n",
    "\n",
    "# Get the most common 2000 words as features\n",
    "word_features = list(all_words)[:2000]\n",
    "\n",
    "# Function to extract features from a document\n",
    "def document_features(document):\n",
    "    document_words = set(document)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[f'contains({word})'] = (word in document_words)\n",
    "    return features\n",
    "\n",
    "# Create feature sets for all documents\n",
    "featuresets = [(document_features(d), c) for (d, c) in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3d052a2-d94c-44f6-8030-01b5aa715fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4: Split the Data into Training and Testing Sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_set, test_set = train_test_split(featuresets, test_size=0.2, random_state=42)\n",
    "\n",
    "# Separate the features and labels\n",
    "X_train, y_train = zip(*train_set)\n",
    "X_test, y_test = zip(*test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da54488-19e1-4130-baa6-a6cc166ed19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 80.25%\n",
      "Most Informative Features\n",
      "   contains(outstanding) = True              pos : neg    =     20.9 : 1.0\n",
      "        contains(seagal) = True              neg : pos    =      7.0 : 1.0\n",
      "         contains(damon) = True              pos : neg    =      6.4 : 1.0\n",
      "        contains(wasted) = True              neg : pos    =      6.4 : 1.0\n",
      "          contains(lame) = True              neg : pos    =      5.7 : 1.0\n",
      "         contains(awful) = True              neg : pos    =      5.1 : 1.0\n",
      "        contains(poorly) = True              neg : pos    =      5.0 : 1.0\n",
      "    contains(ridiculous) = True              neg : pos    =      5.0 : 1.0\n",
      "   contains(wonderfully) = True              pos : neg    =      4.8 : 1.0\n",
      "        contains(allows) = True              pos : neg    =      4.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "#Step 5: Train a Classifier\n",
    "#We will use a Naive Bayes classifier, which is suitable for text classification tasks.\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# Evaluate the classifier\n",
    "print(f'Accuracy: {nltk_accuracy(classifier, test_set) * 100:.2f}%')\n",
    "\n",
    "# Show the most informative features\n",
    "classifier.show_most_informative_features(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a564f-4bc6-47fc-89d8-3b72c3903959",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
