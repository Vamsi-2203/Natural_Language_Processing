{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9c4edaa-33d7-4de5-ac85-fcd805f85278",
   "metadata": {},
   "source": [
    "# Word embedding\n",
    "- Word embedding is a technique used in Natural Language Processing (NLP) to represent words in a continuous vector space where words with similar meanings are mapped closer together.\n",
    "- Unlike traditional methods like Bag of Words (BoW) or TF-IDF, word embeddings capture semantic relationships between words.\n",
    "- These embeddings are usually learned from large corpora of text using neural network models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c518f47f-373b-4259-b943-a1028819f67c",
   "metadata": {},
   "source": [
    "### Key Concepts of Word Embedding\n",
    "- **Continuous Vector Space:** Words are represented as dense vectors of real numbers.\n",
    "- **Semantic Similarity:** Words with similar meanings are close to each other in the vector space.\n",
    "- **Dimensionality Reduction:** Reduces the high-dimensional space of word tokens to a lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe189a5-7e8f-4f76-8a1e-81c2fbebe8de",
   "metadata": {},
   "source": [
    "### Common Word Embedding Techniques\n",
    "Word2Vec: Developed by Google, it uses two main architectures:\n",
    "- **Continuous Bag of Words (CBOW):** Predicts a word based on its context.\n",
    "- **Skip-gram:** Predicts the context given a word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e28be3-841c-476f-8c77-feb253c02fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d3b94ad-2ddd-436e-9cb2-a95df64880a9",
   "metadata": {},
   "source": [
    "# Implementation in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abe42cab-0ab5-4aa7-930c-21ca6fd0c068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation for 'cat':\n",
      " [ 1.30016683e-03 -9.80430283e-03  4.58776252e-03 -5.38222783e-04\n",
      "  6.33209571e-03  1.78347470e-03 -3.12979822e-03  7.75997294e-03\n",
      "  1.55466562e-03  5.52093989e-05 -4.61295387e-03 -8.45352374e-03\n",
      " -7.76683213e-03  8.67050979e-03 -8.92496016e-03  9.03471559e-03\n",
      " -9.28101782e-03 -2.76756298e-04 -1.90704700e-03 -8.93114600e-03\n",
      "  8.63005966e-03  6.77781366e-03  3.01943906e-03  4.83345287e-03\n",
      "  1.12190246e-04  9.42468084e-03  7.02128746e-03 -9.85372625e-03\n",
      " -4.43322072e-03 -1.29011157e-03  3.04772262e-03 -4.32395237e-03\n",
      "  1.44916656e-03 -7.84589909e-03  2.77807354e-03  4.70269192e-03\n",
      "  4.93731257e-03 -3.17570218e-03 -8.42704065e-03 -9.22061782e-03\n",
      " -7.22899451e-04 -7.32746487e-03 -6.81496272e-03  6.12000562e-03\n",
      "  7.17230327e-03  2.11741915e-03 -7.89940078e-03 -5.69898821e-03\n",
      "  8.05184525e-03  3.92084382e-03 -5.24047017e-03 -7.39190448e-03\n",
      "  7.71554711e-04  3.46375466e-03  2.07919348e-03  3.10080405e-03\n",
      " -5.62050007e-03 -9.88948625e-03 -7.02083716e-03  2.30308768e-04\n",
      "  4.61867917e-03  4.52630781e-03  1.87981245e-03  5.17067453e-03\n",
      " -1.05360748e-04  4.11416637e-03 -9.12324060e-03  7.70091172e-03\n",
      "  6.14747405e-03  5.12415636e-03  7.20666908e-03  8.43979698e-03\n",
      "  7.38695846e-04 -1.70386070e-03  5.18628338e-04 -9.31678060e-03\n",
      "  8.40621442e-03 -6.37993217e-03  8.42784252e-03 -4.24435502e-03\n",
      "  6.46842702e-04 -9.16416850e-03 -9.55856778e-03 -7.83681031e-03\n",
      " -7.73105631e-03  3.75581993e-04 -7.22646248e-03 -4.95021325e-03\n",
      " -5.27170673e-03 -4.28929785e-03  7.01231137e-03  4.82938997e-03\n",
      "  8.68277065e-03  7.09359162e-03 -5.69440611e-03  7.24079600e-03\n",
      " -9.29490291e-03 -2.58756871e-03 -7.75716640e-03  4.19260142e-03]\n",
      "Most similar words to 'cat':\n",
      " [('.', 0.21888338029384613), ('friendly', 0.1747603565454483), ('and', 0.16378773748874664), ('my', 0.10853317379951477), ('dog', 0.06548558920621872), ('popular', 0.05959967523813248), ('love', 0.04891003295779228), ('dogs', 0.04767158627510071), ('are', 0.0223334189504385), ('loyal', 0.00640860153362155)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Cats are beautiful animals.\",\n",
    "    \"Dogs are loyal and friendly animals.\",\n",
    "    \"Cats and dogs are popular pets.\",\n",
    "    \"I love my dog.\",\n",
    "    \"My cat is very playful.\"\n",
    "]\n",
    "\n",
    "# Preprocess the documents: tokenize and lower case\n",
    "nltk.download('punkt')\n",
    "tokenized_docs = [nltk.word_tokenize(doc.lower()) for doc in documents]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access the vector for a specific word\n",
    "cat_vector = model.wv['cat']\n",
    "print(\"Vector representation for 'cat':\\n\", cat_vector)\n",
    "\n",
    "# Find the most similar words to 'cat'\n",
    "similar_words = model.wv.most_similar('cat')\n",
    "print(\"Most similar words to 'cat':\\n\", similar_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43c897-36d1-47ff-835a-f57acdedca65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43a7c8a-0226-4b77-a9c3-7bf51e0bba3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7665f1d4-3f9f-4a63-bfad-9250ecd28bf4",
   "metadata": {},
   "source": [
    "The Word2Vec model can be trained using two different algorithms: Continuous Bag of Words (CBOW) and Skip-gram. Both methods are used to predict context in a different manner.\n",
    "\n",
    "- CBOW (Continuous Bag of Words): Predicts the target word (center word) from the context words (surrounding words).\n",
    "- Skip-gram: Predicts the context words from the target word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4984dd9e-ea2f-495e-aa4f-abc7f38e1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Cats are beautiful animals.\",\n",
    "    \"Dogs are loyal and friendly animals.\",\n",
    "    \"Cats and dogs are popular pets.\",\n",
    "    \"I love my dog.\",\n",
    "    \"My cat is very playful.\"\n",
    "]\n",
    "\n",
    "# Tokenize the documents\n",
    "tokenized_docs = [nltk.word_tokenize(doc.lower()) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75c490bf-5287-453a-bf4b-38140b76ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW Vector for 'cat':\n",
      " [ 1.30016683e-03 -9.80430283e-03  4.58776252e-03 -5.38222783e-04\n",
      "  6.33209571e-03  1.78347470e-03 -3.12979822e-03  7.75997294e-03\n",
      "  1.55466562e-03  5.52093989e-05 -4.61295387e-03 -8.45352374e-03\n",
      " -7.76683213e-03  8.67050979e-03 -8.92496016e-03  9.03471559e-03\n",
      " -9.28101782e-03 -2.76756298e-04 -1.90704700e-03 -8.93114600e-03\n",
      "  8.63005966e-03  6.77781366e-03  3.01943906e-03  4.83345287e-03\n",
      "  1.12190246e-04  9.42468084e-03  7.02128746e-03 -9.85372625e-03\n",
      " -4.43322072e-03 -1.29011157e-03  3.04772262e-03 -4.32395237e-03\n",
      "  1.44916656e-03 -7.84589909e-03  2.77807354e-03  4.70269192e-03\n",
      "  4.93731257e-03 -3.17570218e-03 -8.42704065e-03 -9.22061782e-03\n",
      " -7.22899451e-04 -7.32746487e-03 -6.81496272e-03  6.12000562e-03\n",
      "  7.17230327e-03  2.11741915e-03 -7.89940078e-03 -5.69898821e-03\n",
      "  8.05184525e-03  3.92084382e-03 -5.24047017e-03 -7.39190448e-03\n",
      "  7.71554711e-04  3.46375466e-03  2.07919348e-03  3.10080405e-03\n",
      " -5.62050007e-03 -9.88948625e-03 -7.02083716e-03  2.30308768e-04\n",
      "  4.61867917e-03  4.52630781e-03  1.87981245e-03  5.17067453e-03\n",
      " -1.05360748e-04  4.11416637e-03 -9.12324060e-03  7.70091172e-03\n",
      "  6.14747405e-03  5.12415636e-03  7.20666908e-03  8.43979698e-03\n",
      "  7.38695846e-04 -1.70386070e-03  5.18628338e-04 -9.31678060e-03\n",
      "  8.40621442e-03 -6.37993217e-03  8.42784252e-03 -4.24435502e-03\n",
      "  6.46842702e-04 -9.16416850e-03 -9.55856778e-03 -7.83681031e-03\n",
      " -7.73105631e-03  3.75581993e-04 -7.22646248e-03 -4.95021325e-03\n",
      " -5.27170673e-03 -4.28929785e-03  7.01231137e-03  4.82938997e-03\n",
      "  8.68277065e-03  7.09359162e-03 -5.69440611e-03  7.24079600e-03\n",
      " -9.29490291e-03 -2.58756871e-03 -7.75716640e-03  4.19260142e-03]\n",
      "Most similar words to 'cat' (CBOW):\n",
      " [('.', 0.21888338029384613), ('friendly', 0.1747603565454483), ('and', 0.16378773748874664), ('my', 0.10853317379951477), ('dog', 0.06548558920621872), ('popular', 0.05959967523813248), ('love', 0.04891003295779228), ('dogs', 0.04767158627510071), ('are', 0.0223334189504385), ('loyal', 0.00640860153362155)]\n"
     ]
    }
   ],
   "source": [
    "#CBOW Implementation\n",
    "#In gensim, setting the sg parameter to 0 will use the CBOW model.\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train CBOW model\n",
    "cbow_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
    "\n",
    "# Access the vector for a specific word\n",
    "cat_vector_cbow = cbow_model.wv['cat']\n",
    "print(\"CBOW Vector for 'cat':\\n\", cat_vector_cbow)\n",
    "\n",
    "# Find the most similar words to 'cat'\n",
    "similar_words_cbow = cbow_model.wv.most_similar('cat')\n",
    "print(\"Most similar words to 'cat' (CBOW):\\n\", similar_words_cbow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047b1723-fbca-4a31-85a3-d9c15dc13408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip-gram Vector for 'cat':\n",
      " [ 1.30016683e-03 -9.80430283e-03  4.58776252e-03 -5.38222783e-04\n",
      "  6.33209571e-03  1.78347470e-03 -3.12979822e-03  7.75997294e-03\n",
      "  1.55466562e-03  5.52093989e-05 -4.61295387e-03 -8.45352374e-03\n",
      " -7.76683213e-03  8.67050979e-03 -8.92496016e-03  9.03471559e-03\n",
      " -9.28101782e-03 -2.76756298e-04 -1.90704700e-03 -8.93114600e-03\n",
      "  8.63005966e-03  6.77781366e-03  3.01943906e-03  4.83345287e-03\n",
      "  1.12190246e-04  9.42468084e-03  7.02128746e-03 -9.85372625e-03\n",
      " -4.43322072e-03 -1.29011157e-03  3.04772262e-03 -4.32395237e-03\n",
      "  1.44916656e-03 -7.84589909e-03  2.77807354e-03  4.70269192e-03\n",
      "  4.93731257e-03 -3.17570218e-03 -8.42704065e-03 -9.22061782e-03\n",
      " -7.22899451e-04 -7.32746487e-03 -6.81496272e-03  6.12000562e-03\n",
      "  7.17230327e-03  2.11741915e-03 -7.89940078e-03 -5.69898821e-03\n",
      "  8.05184525e-03  3.92084382e-03 -5.24047017e-03 -7.39190448e-03\n",
      "  7.71554711e-04  3.46375466e-03  2.07919348e-03  3.10080405e-03\n",
      " -5.62050007e-03 -9.88948625e-03 -7.02083716e-03  2.30308768e-04\n",
      "  4.61867917e-03  4.52630781e-03  1.87981245e-03  5.17067453e-03\n",
      " -1.05360748e-04  4.11416637e-03 -9.12324060e-03  7.70091172e-03\n",
      "  6.14747405e-03  5.12415636e-03  7.20666908e-03  8.43979698e-03\n",
      "  7.38695846e-04 -1.70386070e-03  5.18628338e-04 -9.31678060e-03\n",
      "  8.40621442e-03 -6.37993217e-03  8.42784252e-03 -4.24435502e-03\n",
      "  6.46842702e-04 -9.16416850e-03 -9.55856778e-03 -7.83681031e-03\n",
      " -7.73105631e-03  3.75581993e-04 -7.22646248e-03 -4.95021325e-03\n",
      " -5.27170673e-03 -4.28929785e-03  7.01231137e-03  4.82938997e-03\n",
      "  8.68277065e-03  7.09359162e-03 -5.69440611e-03  7.24079600e-03\n",
      " -9.29490291e-03 -2.58756871e-03 -7.75716640e-03  4.19260142e-03]\n",
      "Most similar words to 'cat' (Skip-gram):\n",
      " [('.', 0.21884268522262573), ('friendly', 0.1747603565454483), ('and', 0.16378773748874664), ('my', 0.10853002965450287), ('dog', 0.06548558920621872), ('popular', 0.05959967523813248), ('love', 0.04890257120132446), ('dogs', 0.04767158627510071), ('are', 0.022314250469207764), ('loyal', 0.00640860153362155)]\n"
     ]
    }
   ],
   "source": [
    "#Skip-gram Implementation\n",
    "#In gensim, setting the sg parameter to 1 will use the Skip-gram model.\n",
    "\n",
    "# Train Skip-gram model\n",
    "skipgram_model = Word2Vec(sentences=tokenized_docs, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Access the vector for a specific word\n",
    "cat_vector_skipgram = skipgram_model.wv['cat']\n",
    "print(\"Skip-gram Vector for 'cat':\\n\", cat_vector_skipgram)\n",
    "\n",
    "# Find the most similar words to 'cat'\n",
    "similar_words_skipgram = skipgram_model.wv.most_similar('cat')\n",
    "print(\"Most similar words to 'cat' (Skip-gram):\\n\", similar_words_skipgram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c44c3ce-106a-48ff-bb21-4fbf4217c23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50470b4-58b8-4220-a8a1-92e7f196297b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4ca0ca2-d4ac-4719-9adc-f9dc5b54dab8",
   "metadata": {},
   "source": [
    "#### Differences Between CBOW and Skip-gram\n",
    "- **CBOW:** Tends to be faster and works well with smaller datasets. It smooths over a lot of distributional information.\n",
    "- **Skip-gram:** Works better with larger datasets and captures finer semantic relationships. It is more accurate for infrequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee9a7d2-f0a6-4e15-ab04-92769b1579c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
